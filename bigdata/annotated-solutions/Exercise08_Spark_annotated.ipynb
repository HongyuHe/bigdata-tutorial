{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ycwd1jWj2E8T"
   },
   "source": [
    "# <center>Big Data &ndash; Exercises </center>\n",
    "## <center>Fall 2023 &ndash; Week 8 &ndash; ETH Zurich</center>\n",
    "## <center>YARN + Spark</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cdbi0gZS2E8V"
   },
   "source": [
    "# Exercise 1 &mdash; What is YARN?\n",
    "\n",
    "Fundamentally, “**Y**et **A**nother **R**esource **N**egotiator”. **YARN**  is a resource scheduler designed to work on existing and new Hadoop clusters. \n",
    "\n",
    "YARN supports pluggable schedulers. The task of the scheduler is to share the resources of a large cluster among different tenants (applications) while trying to meet application demands (memory, CPU). A user may have several applications active at a time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RNt4v5cG2E8V"
   },
   "source": [
    "### 1.1 &ndash; List at least 3 main shortcomings of MapReduce v1, which are addressed by YARN design."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WXFe-GA62E8W"
   },
   "source": [
    "### Answer\n",
    "\n",
    "1. **Scalability Limitations in MapReduce v1**:\n",
    "   - *Bottleneck*: MapReduce v1 suffered from a bottleneck with the JobTracker, responsible for resource scheduling and monitoring. This limited scalability, allowing for only around 5K nodes and up to 40K concurrent tasks (per Yahoo!).\n",
    "   - *YARN Solution*: YARN's design decentralizes resource management, eliminating the JobTracker bottleneck. It allows for much greater scalability by enabling efficient resource allocation and management across the cluster, accommodating larger node counts (~10K) and higher concurrent task loads.\n",
    "\n",
    "2. **Rigidity in Job Types**:\n",
    "   - *MapReduce Specificity*: MapReduce v1 was confined to supporting only MapReduce-specific jobs. This limitation hindered the ability to schedule and execute diverse workloads such as MPI, graph processing, or user-specific code.\n",
    "   - *YARN's Adaptability*: YARN's architecture is designed to accommodate various workloads beyond MapReduce, enabling the sharing of the cluster resources among different types of applications and frameworks. It allows for scheduling and executing non-MapReduce workloads like MPI, graph processing, and custom user code, enhancing the cluster's versatility.\n",
    "\n",
    "3. **Resource Utilization Efficiency**:\n",
    "   - *Idle Resources*: MapReduce v1 exhibited inefficiencies in resource utilization where reducers often waited for mappers to finish (and vice-versa), resulting in idle resources during these wait times.\n",
    "   - *YARN's Dynamic Resource Allocation*: YARN optimizes resource utilization by allowing independent scheduling of resources for different tasks. It enables simultaneous execution of various tasks, ensuring that all available resources are utilized efficiently at any given time, reducing idle periods and maximizing overall cluster performance.\n",
    "\n",
    "4. **Flexibility and Dynamic Configuration**:\n",
    "   - *Static Mapper and Reducer Roles*: MapReduce v1 constrained the roles of mappers and reducers to configurations set at job initiation, lacking the flexibility to adapt roles during runtime.\n",
    "   - *YARN's Dynamic Configuration*: YARN provides greater flexibility by allowing dynamic allocation and reallocation of resources during runtime. Applications can adapt their resource requirements and configurations dynamically, allowing for more efficient resource utilization and adaptive task handling based on changing workloads or conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Components of YARN\n",
    "\n",
    "\n",
    "1. **ResourceManager (RM):**\n",
    "   - **Global Resource Manager**: Controls and manages the cluster's resources.\n",
    "   - **Resource Allocation**: Allocates resources to various applications.\n",
    "   - **Scheduling**: Schedules resources among competing applications based on policies and priorities.\n",
    "   - **Manages NodeManagers**: Monitors NodeManagers and their resource availability.\n",
    "\n",
    "2. **NodeManager (NM):**\n",
    "   - **Node-level Resource Manager**: Manages resources on individual cluster nodes.\n",
    "   - **Monitors Resource Usage**: Tracks resource utilization (CPU, memory, etc.) on the node.\n",
    "   - **Execution and Monitoring**: Executes and monitors containers (fundamental units of execution) on the node.\n",
    "   - **Reports to ResourceManager**: Regularly communicates with the ResourceManager, providing updates on available resources and container status.\n",
    "\n",
    "3. **ApplicationMaster (AM):**\n",
    "   - **Per-Application Coordinator**: Manages a specific application running on YARN.\n",
    "   - **Resource Negotiation**: Negotiates resources from the ResourceManager for the application's tasks or containers.\n",
    "   - **Application Lifecycle Management**: Coordinates the execution lifecycle, handling task execution, monitoring progress, and handling failures or retries.\n",
    "   - **Optimizes Execution**: Optimizes task execution based on resource availability and application requirements.\n",
    "\n",
    "4. **Containers:**\n",
    "   - **Execution Units**: Containers encapsulate allocated resources (CPU, memory, etc.) for running specific tasks or components of an application.\n",
    "   - **Isolated Execution Environment**: Provides a controlled environment for executing tasks without interference from other containers on the same node.\n",
    "   - **Managed by NodeManager**: Managed and executed by the NodeManager on respective nodes.\n",
    "\n",
    "5. **ResourceManager Web UI:**\n",
    "   - **Visualization and Monitoring**: Provides a web-based interface for administrators and users to monitor resource utilization, application status, and cluster health.\n",
    "   - **Insight into Resource Allocation**: Offers insights into resource allocation, application progress, and overall cluster performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps taken to launch a job in YARN\n",
    "\n",
    "\n",
    "1. **Submitting the Job:**\n",
    "   - A user submits their job/application to the YARN cluster. This submission might be through a command-line interface, an API call, or a higher-level framework built on top of YARN, such as Apache Hadoop MapReduce, Apache Spark, or others.\n",
    "\n",
    "2. **ResourceManager Receives the Request:**\n",
    "   - The ResourceManager (RM) receives the job/application submission request.\n",
    "   - The RM is responsible for managing the cluster's resources and allocating resources to various applications.\n",
    "\n",
    "3. **Resource Negotiation:**\n",
    "   - The ResourceManager negotiates resources for the submitted job/application.\n",
    "   - It considers the resource requirements specified by the user or the job/application (e.g., memory, CPU cores) and the current availability of resources in the cluster.\n",
    "\n",
    "4. **Allocation of Containers:**\n",
    "   - Upon resource allocation, the ResourceManager assigns containers to the ApplicationMaster (AM) for the submitted job/application.\n",
    "   - Each container encapsulates the allocated resources (CPU, memory, etc.) needed to execute specific tasks.\n",
    "\n",
    "5. **Launching the ApplicationMaster:**\n",
    "   - The ResourceManager launches the ApplicationMaster on a chosen node in the cluster.\n",
    "   - The ApplicationMaster is responsible for managing the execution of the user's job/application. It coordinates with the ResourceManager for resource needs and oversees task execution.\n",
    "\n",
    "6. **Task Execution within Containers:**\n",
    "   - The ApplicationMaster, once initialized, starts sending requests to the NodeManagers (NMs) to launch containers on specific nodes within the cluster.\n",
    "   - NodeManagers launch containers and execute tasks associated with the job/application within these containers.\n",
    "\n",
    "7. **Container Execution and Monitoring:**\n",
    "   - NodeManagers monitor and manage the execution of containers.\n",
    "   - They _report container status updates (start, finish, failure) to the ResourceManager, which in turn communicates this information to the ApplicationMaster._\n",
    "\n",
    "8. **Task Completion and Job Termination:**\n",
    "   - As tasks within containers complete their execution, they report their status back to the ApplicationMaster.\n",
    "   - The ApplicationMaster keeps track of task completion, aggregates results, and manages the overall progress of the job/application.\n",
    "\n",
    "9. **Finalization and Resource Release:**\n",
    "   - Once the job/application completes its tasks, the ApplicationMaster signals the ResourceManager about job completion.\n",
    "   - The ResourceManager releases the allocated resources back to the cluster for other applications and awaits further job submissions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1WJJEGE12E8X"
   },
   "source": [
    "### 1.2 &ndash; State which of the following statements are true:\n",
    "\n",
    "1. The ResourceManager has to provide fault tolerance for resources across the cluster \n",
    "\n",
    "    The statement is __false__ because, in the context of YARN (Yet Another Resource Negotiator), the responsibility for fault tolerance of resources across the cluster does not lie primarily with the ResourceManager. Fault tolerance in YARN is primarily handled by other components:\n",
    "\n",
    "    1. **NodeManagers**: These are responsible for managing resources on individual nodes in the cluster. They monitor resource usage, report to the ResourceManager, and manage container execution. NodeManagers are crucial in handling failures at the node level and ensuring that tasks or containers are rerun on other healthy nodes in case of node failures.\n",
    "\n",
    "    2. **ApplicationMaster**: Each application running on YARN has its own ApplicationMaster. The ApplicationMaster is responsible for negotiating resources with the ResourceManager and managing the execution of the application's tasks or containers. It monitors the progress of the application and handles failures or retries at the application level.\n",
    "\n",
    "    3. **HDFS**: For fault tolerance of data, YARN often relies on the underlying HDFS. HDFS replicates data across multiple nodes in the cluster to ensure that data remains available even if some nodes fail.\n",
    "\n",
    "\n",
    "1. Container allocation/deallocation can take place in a dynamic fashion as the application progresses. \n",
    "\n",
    "    __True__\n",
    "\n",
    "1. YARN plans to allow applications to only request resources in terms of memory usage and number of CPUs.\n",
    "    \n",
    "    __False__, because because YARN, as a resource management framework, offers flexibility in the type of resources that applications can request beyond just memory and CPU.\n",
    "\n",
    "    YARN is designed to accommodate diverse workloads and varied resource requirements beyond memory and CPU. It supports the notion of \"resource types,\" which enables applications to request and utilize different kinds of resources based on their specific needs. These resource types can include but are not limited to:\n",
    "\n",
    "    1. **Memory:** Applications can request a certain amount of memory for their tasks or containers.\n",
    "    2. **CPU:** They can also specify the number of CPU cores required for their execution.\n",
    "    3. **GPUs:** For applications that benefit from GPU acceleration, YARN allows for GPU resource requests.\n",
    "    4. **Disk Space:** Applications might need specific disk space for temporary storage or data processing.\n",
    "    5. **Custom Resources:** YARN allows for defining custom resource types to cater to specific application requirements that go beyond standard memory and CPU, such as specialized hardware or software licenses.\n",
    "\n",
    "    By enabling applications to request various types of resources, YARN supports a wide range of workloads, including machine learning, data analytics, scientific computing, and more, each with distinct resource needs beyond memory and CPU. Therefore, the statement is false because YARN's design is not limited solely to memory usage and the number of CPUs when it comes to resource requests.\n",
    "\n",
    "1. Communications between the ResourceManager and NodeManagers are heartbeat-based. \n",
    "\n",
    "    __True__:\n",
    "    NodeManagers periodically send heartbeats (every a few seconds) to the ResourceManager to provide status updates and information about the resources available on their respective nodes. These heartbeats serve as a way for NodeManagers to communicate their health, report resource utilization, and update the ResourceManager about the status of containers they are managing.\n",
    "\n",
    "    The ResourceManager relies on these heartbeat messages from NodeManagers to keep track of node health, available resources, and to detect any issues or failures at the node level. By maintaining a regular heartbeat exchange, the ResourceManager can dynamically manage resources, allocate or reallocate resources as needed, and make decisions based on the most up-to-date information about the cluster's state.\n",
    "    This heartbeat-based communication ensures that the ResourceManager stays informed about the health and status of the nodes in the cluster, allowing for effective resource allocation, fault tolerance, and overall cluster management within the YARN framework.\n",
    "\n",
    "1. The ResourceManager does not have a global view of all usage of cluster resources. Therefore, it tries to make better scheduling decisions based on probabilistic prediction. \n",
    "\n",
    "    __False__ (last session)\n",
    "\n",
    "1. ResourceManager has the ability to request resources back from a running application.\n",
    "\n",
    "    __True__: In the YARN (Yet Another Resource Negotiator) framework, the ResourceManager (RM) has the capability to reclaim or request resources back from a running application under certain circumstances.\n",
    "\n",
    "    This feature is known as \"resource preemption.\" Preemption allows the ResourceManager to reclaim resources from applications that are utilizing more resources than their fair share or if there's a need to allocate resources to higher-priority applications.\n",
    "\n",
    "    1. **Queue-based Preemption:** YARN supports resource allocation based on queues with varying priorities. If a higher-priority queue or application needs resources that are currently being used by a lower-priority application, the ResourceManager can reclaim resources from the lower-priority application to satisfy the higher-priority request.\n",
    "\n",
    "    2. **Over-allocation or Fairness:** If an application is using resources significantly beyond its allocated share or if it's causing other applications to be starved of resources, the ResourceManager can step in and reclaim resources to maintain fairness and prevent resource monopolization.\n",
    "\n",
    "    3. **Dynamic Resource Demands:** In environments where resource demands fluctuate dynamically, the ResourceManager can adjust resource allocations across applications to ensure optimal resource utilization and meet changing demands efficiently.\n",
    "\n",
    "    Resource preemption ensures that resources are distributed fairly and efficiently among competing applications in the cluster. However, preemption is typically used judiciously to avoid disrupting long-running jobs or causing unnecessary interference with the application's execution, aiming to balance fairness and cluster efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hGYiOFom2E8Y",
    "tags": []
   },
   "source": [
    "### 1.3 &ndash; Whose responsibility is it? Say which component of YARN is resposible for each of the following tasks.\n",
    "\n",
    "1. Fault Tolerance of running applications *[ResourceManager | ApplicationMaster | NodeManager ]*\n",
    "    \n",
    "    AM\n",
    "    \n",
    "1. Asking for resources needed for an application *[ResourceManager | ApplicationMaster | NodeManager ]*\n",
    "    \n",
    "    AM\n",
    "\n",
    "1. Providing leases to use containers *[ResourceManager | ApplicationMaster | NodeManager]*\n",
    "\n",
    "    RM\n",
    "\n",
    "1. Tracking status and progress of running applications *[ResourceManager | ApplicationMaster | NodeManager]*\n",
    "\n",
    "    AM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Bonus__: Handling container failures. *[ResourceManager | ApplicationMaster | NodeManager ]*\n",
    "\n",
    "RM + AM + NM\n",
    "\n",
    "1. **Container Failure Detection:** When a container fails, the NodeManager detects the failure and reports it to the ResourceManager.\n",
    "\n",
    "2. **ResourceManager's Role:** The ResourceManager receives information about container failures and manages the cluster's resources. It reallocates resources and attempts to recover from the failure by potentially rerunning the failed container on a different node if necessary. _The ResourceManager ensures that the resources requested by the ApplicationMaster are made available for the failed task to be retried._\n",
    "\n",
    "3. **ApplicationMaster's Response:** The ApplicationMaster, which is responsible for managing the specific application's execution, receives updates from the ResourceManager about container status changes. Upon detecting a container failure, the ApplicationMaster can take action, such as:\n",
    "\n",
    "   - **Retrying Failed Tasks:** The ApplicationMaster might initiate a retry of the failed task by requesting the ResourceManager to allocate resources for a new container to replace the failed one. This retry can occur on the same or a different node.\n",
    "   \n",
    "   - **Handling Failures:** The ApplicationMaster might implement error handling strategies, such as logging the failure, updating the job status, resuming from a checkpoint, or taking other appropriate actions based on the application's logic and requirements.\n",
    "\n",
    "YARN's design allows for targeted recovery by rerunning specific failed tasks or containers, minimizing the impact of failures on the overall job execution. The ApplicationMaster is responsible for managing these recovery actions based on the status updates received from the ResourceManager."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BQTrigNm2E8Z"
   },
   "source": [
    "### 1.4 &ndash; What is the typical configuration for YARN? Choose for the following components how many instances of them there are in a cluster.\n",
    "\n",
    "```\n",
    "1. ResourceManager -- a                a. One per cluster\n",
    "\n",
    "2. ApplicationMaster -- c                b. One per node\n",
    "\n",
    "3. NodeManager -- b                      c. Many per cluster, but usually not per node\n",
    "\n",
    "4. Container -- d                        d. Many per node \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "df9Arowc2E8e"
   },
   "source": [
    "## 2. Setup the Spark cluster on Docker\n",
    "\n",
    "1. Start docker <br> \n",
    "   docker-compose up -d\n",
    "\n",
    "2. Access jupyter notebook <br> \n",
    "   http://localhost:8888/lab/tree/work/Exercise08_Spark.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sj15DNuA2E8f"
   },
   "source": [
    "## 3. Apache Spark Architecuture\n",
    "\n",
    "Spark is a cluster computing platform designed to be fast and general purpose. Spark extends the MapReduce model to efficiently cover a wide range of workloads that previously required separate distributed systems, including interactive queries and stream processing. Spark offers the ability to run computations in memory.\n",
    "\n",
    "At a high level, every Spark application consists of a **driver program** that launches various parallel operations on a cluster. The driver program contains your application's main function and defines distributed datasets on the cluster, then applies operations to them.\n",
    "\n",
    "Driver programs access Spark through a **SparkContext** object, which represents a connection to a computing cluster. There is no need to create a SparkContext; it is created for you automatically when you run the first code cell in the Jupyter\n",
    "\n",
    "The driver communicates with a potentially large number of distributed workers called **executors**. The driver runs in its own process and each executor is a separate process. A driver and its executors are together termed a Spark application.\n",
    "\n",
    "![Image of Account](http://spark.apache.org/docs/latest/img/cluster-overview.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N6K2sJlR2E8f"
   },
   "source": [
    "### 3.1 Understand resilient distributed datasets (RDD)\n",
    "\n",
    "An RDD in Spark is simply an immutable distributed collection of objects. Each RDD is split into multiple partitions, which may be computed on different nodes of the cluster. \n",
    "\n",
    "##### What are RDD operations?\n",
    "RDDs offer two types of operations: **transformations** and **actions**.\n",
    "\n",
    "* **Transformations** create a new dataset from an existing one. Transformations are lazy, meaning that no transformation is executed until you execute an action.\n",
    "* **Actions** compute a result based on an RDD, and either return it to the driver program or save it to an external storage system (e.g., HDFS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Su_8K5JS2E8g"
   },
   "source": [
    "Transformations and actions are different because of the way Spark computes RDDs. Although you can define new RDDs any time, Spark computes them only in a **lazy** fashion, that is, the first time they are used in an action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1zVeNxBR2E8g"
   },
   "source": [
    "##### Create Spark session (Spark install within jupyter docker image) and context\n",
    "\n",
    "RDDs can be created from stable storage or by transforming other RDDs. Run the cells below to create RDDs from the sample data files that have been copied to hdfs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why was RDD designed to be immutable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why was RDD designed to be \"lazy\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why is Spark much faster than MapReduce?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark outperforms Hadoop MapReduce in several aspects, leading to its faster processing speed and improved performance:\n",
    "\n",
    "1. **In-Memory Processing:** Spark performs computations in memory, reducing the need to read and write to disk after each step, unlike Hadoop MapReduce, which writes to disk between map and reduce phases. This in-memory processing significantly reduces I/O overhead and speeds up data processing.\n",
    "\n",
    "2. **DAG Execution Model:** Spark uses a Directed Acyclic Graph (DAG) execution model, allowing it to optimize and chain together multiple operations or transformations into a single job. This eliminates the overhead of writing intermediate results to disk after each stage, improving efficiency.\n",
    "\n",
    "3. **Resilient Distributed Datasets (RDDs):** Spark's RDDs are a fundamental abstraction that allows for fault tolerance and distributed data processing. RDDs can be cached in memory across multiple operations, reducing data replication and improving the speed of subsequent computations.\n",
    "\n",
    "4. **Lazy Evaluation:** Spark uses lazy evaluation, deferring the actual execution of operations until an action is triggered. This optimization allows it to optimize the execution plan and minimize unnecessary computations, resulting in faster processing.\n",
    "\n",
    "5. **Rich APIs and Libraries:** Spark offers a wide range of high-level APIs in Python, Scala, Java, and R, along with various libraries (e.g., Spark SQL, MLlib, GraphX), enabling diverse and complex data processing tasks to be performed more efficiently compared to the limited capabilities of MapReduce.\n",
    "\n",
    "6. **Interactive and Iterative Processing:** Spark is well-suited for interactive data analysis and iterative machine learning algorithms due to its in-memory computations and iterative processing capabilities, resulting in faster response times for these workloads compared to MapReduce.\n",
    "\n",
    "7. **Dynamic Partitioning and Pipelining:** Spark provides dynamic partitioning, enabling users to control how data is distributed across nodes, and pipelining optimizations that minimize shuffling and data movement, leading to improved performance.\n",
    "\n",
    "While Spark demonstrates faster performance compared to Hadoop MapReduce in many scenarios, the choice between the two depends on specific use cases, data sizes, resource availability, and processing requirements. In certain situations, such as handling large-scale batch processing or when fault tolerance is critical, MapReduce might still be a suitable choice despite Spark's performance advantages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "#* use spark session\n",
    "spark = SparkSession.builder.master('local').getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 2390.4140625,
      "end_time": 1604569551027.697
     }
    },
    "id": "coRjk8iA2E8g",
    "outputId": "95d2ab62-4891-48a2-ddc9-cd11fa42286a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create an RDD\n",
    "fruits = sc.textFile('fruits.txt')\n",
    "yellowThings = sc.textFile('yellowthings.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "APVFv9Ps2E8j"
   },
   "source": [
    "##### RDD transformations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 309.608154296875,
      "end_time": 1604570982369.364
     }
    },
    "id": "atWh0L_e2E8k",
    "outputId": "aaa2337e-0d43-4f03-e90c-531c3c2652ff",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['elppa',\n",
       " 'ananab',\n",
       " 'nolem yranac',\n",
       " 'eparg',\n",
       " 'nomel',\n",
       " 'egnaro',\n",
       " 'elppaenip',\n",
       " 'yrrebwarts']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# map\n",
    "fruitsReversed = fruits.map(lambda fruit: fruit[::-1])\n",
    "fruitsReversed.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 261.870849609375,
      "end_time": 1604570982657.437
     }
    },
    "id": "PT7_eUQR2E8m",
    "outputId": "ce3919e0-99c1-44db-ec38-a9bdb79d13f3",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['apple', 'grape', 'lemon']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter\n",
    "shortFruits = fruits.filter(lambda fruit: len(fruit) <= 5)\n",
    "shortFruits.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 289.953857421875,
      "end_time": 1604570982970.662
     }
    },
    "id": "Oi_FWVI_2E8o",
    "outputId": "d0cfae93-ee9e-4151-9b33-f385448b32ca",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'p', 'p', 'l', 'e', 'g', 'r', 'a', 'p', 'e', 'l', 'e', 'm', 'o', 'n']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flatMap\n",
    "characters = (fruits\n",
    "                  .filter(lambda fruit: len(fruit) <= 5)\n",
    "                  .flatMap(lambda fruit: list(fruit)))\n",
    "characters.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 820.903076171875,
      "end_time": 1604570983811.643
     }
    },
    "id": "wRKKpIkl2E8q",
    "outputId": "2c4ae0f0-aeb1-499b-8186-12c4de8b8f39",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['apple',\n",
       " 'banana',\n",
       " 'canary melon',\n",
       " 'grape',\n",
       " 'lemon',\n",
       " 'orange',\n",
       " 'pineapple',\n",
       " 'strawberry',\n",
       " 'banana',\n",
       " 'bee',\n",
       " 'butter',\n",
       " 'canary melon',\n",
       " 'gold',\n",
       " 'lemon',\n",
       " 'pineapple',\n",
       " 'sunflower']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# union between fruits and yellowthings datasets\n",
    "fruitsAndYellowThings = fruits.union(yellowThings)\n",
    "fruitsAndYellowThings.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 810.952880859375,
      "end_time": 1604570984650.106
     }
    },
    "id": "EsrYtoro2E8s",
    "outputId": "264fe33f-25e3-448a-d6f6-234fe1183bd3",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['banana', 'pineapple', 'canary melon', 'lemon']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# intersection between fruits and yellowthings datasets\n",
    "yellowFruits = fruits.intersection(yellowThings)\n",
    "yellowFruits.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 777.156005859375,
      "end_time": 1604570985454.588
     }
    },
    "id": "fq8rLaoc2E8u",
    "outputId": "c63df4eb-5d92-4181-9eb3-4da8ecea9dcc",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['banana',\n",
       " 'orange',\n",
       " 'pineapple',\n",
       " 'butter',\n",
       " 'gold',\n",
       " 'sunflower',\n",
       " 'apple',\n",
       " 'canary melon',\n",
       " 'grape',\n",
       " 'lemon',\n",
       " 'strawberry',\n",
       " 'bee']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# distinct elements in the two datasets\n",
    "distinctFruitsAndYellowThings = fruitsAndYellowThings.distinct()\n",
    "distinctFruitsAndYellowThings.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 800.93798828125,
      "end_time": 1604570986276.183
     }
    },
    "id": "cRGqpVgO2E8w",
    "outputId": "e18e313f-86e7-4e42-bf67-af5ac4720ab3",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words starting with:  b\n",
      " >  banana\n",
      " >  bee\n",
      " >  butter\n",
      "Words starting with:  c\n",
      " >  canary melon\n",
      "Words starting with:  g\n",
      " >  gold\n",
      "Words starting with:  l\n",
      " >  lemon\n",
      "Words starting with:  p\n",
      " >  pineapple\n",
      "Words starting with:  s\n",
      " >  sunflower\n"
     ]
    }
   ],
   "source": [
    "# groupByKey\n",
    "yellowThingsByFirstLetter = yellowThings.map(lambda thing: (thing[0], thing)).groupByKey()\n",
    "for letter, lst in yellowThingsByFirstLetter.collect():\n",
    "    print(\"Words starting with: \", letter)\n",
    "    for obj in lst:\n",
    "        print(\" > \", obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 777.8681640625,
      "end_time": 1604570987091.685
     }
    },
    "id": "YUjWL3Jp2E8y",
    "outputId": "df04cb76-bbc6-44e8-ca79-ec7809e12006",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5, 3), (6, 2), (12, 1), (9, 1), (10, 1)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MapReduce: key is the number of characters of the fruit name (len(fruit))\n",
    "numFruitsByLength = fruits.map(lambda fruit: (len(fruit), 1)).reduceByKey(lambda a, b: a + b)\n",
    "numFruitsByLength.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7MIj-ytt2E8z"
   },
   "source": [
    "##### RDD actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 279.06396484375,
      "end_time": 1604570987396.644
     }
    },
    "id": "rwqMc4iZ2E80",
    "outputId": "6409fad4-7979-4e9d-c83b-44238373872c",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple', 'banana', 'canary melon', 'grape', 'lemon', 'orange', 'pineapple', 'strawberry']\n",
      "['banana', 'bee', 'butter', 'canary melon', 'gold', 'lemon', 'pineapple', 'sunflower']\n"
     ]
    }
   ],
   "source": [
    "# collect\n",
    "fruitsArray = fruits.collect()\n",
    "yellowThingsArray = yellowThings.collect()\n",
    "print(fruitsArray)\n",
    "print(yellowThingsArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 281.281982421875,
      "end_time": 1604570987702.166
     }
    },
    "id": "UvyrLdxN2E82",
    "outputId": "845b0f07-3743-407a-9192-5cd9bff9063d",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count - how many fruits are\n",
    "fruits.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 312.892822265625,
      "end_time": 1604570988042.097
     }
    },
    "id": "j274FPSm2E84",
    "outputId": "3e3ce495-e519-47c2-cc1c-4ae6b0a2cc3d",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['apple', 'banana', 'canary melon']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take - show the first three fruits\n",
    "fruits.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 282.5498046875,
      "end_time": 1604570988345.706
     }
    },
    "id": "g5DOneF42E86",
    "outputId": "c897dc70-1691-4f2f-b46d-a52271eccfd3",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'e',\n",
       " 'g',\n",
       " 'i',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'w',\n",
       " 'y'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reduce - what letters are used\n",
    "fruits.map(lambda fruit: set(fruit)).reduce(lambda x, y: x.union(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5nSN1JhN2E88"
   },
   "source": [
    "##### Lazy evaluation\n",
    "Lazy evaluation means that when we call a transformation on an RDD (for instance, calling `map()`), the operation is not immediately performed. Instead, Spark internally records metadata to indicate that this operation has been requested. Rather than thinking of an RDD as containing specific data, it is best to think of each RDD as\n",
    "consisting of instructions on how to compute the data that we build up through transformations. Loading data into an RDD is lazily evaluated in the same way transformations are. So, when we call `sc.textFile()`, the data is not loaded until it is necessary. As with transformations, the operation (in this case, reading the data) can\n",
    "occur multiple times.\n",
    "\n",
    "\n",
    "Finally, as you derive new RDDs from each other using transformations, Spark keeps track of the set of dependencies between different RDDs, called the lineage graph. For instance, the code bellow corresponds to the following graph:\n",
    "\n",
    "<img src=\"https://cloud.inf.ethz.ch/s/BiRkaa97xKEZ7NE/download\" height=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 276.652099609375,
      "end_time": 1604572082649.72
     }
    },
    "id": "Tg8ZBO5L2E88",
    "outputId": "3a0f651c-e5a7-4a9b-f51f-12e10884bb5e",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple', 'pineapple', 'lemon']\n",
      "(2) UnionRDD[48] at union at NativeMethodAccessorImpl.java:0 []\n",
      " |  PythonRDD[46] at RDD at PythonRDD.scala:53 []\n",
      " |  fruits.txt MapPartitionsRDD[16] at textFile at NativeMethodAccessorImpl.java:0 []\n",
      " |  fruits.txt HadoopRDD[15] at textFile at NativeMethodAccessorImpl.java:0 []\n",
      " |  PythonRDD[47] at RDD at PythonRDD.scala:53 []\n",
      " |  yellowthings.txt MapPartitionsRDD[18] at textFile at NativeMethodAccessorImpl.java:0 []\n",
      " |  yellowthings.txt HadoopRDD[17] at textFile at NativeMethodAccessorImpl.java:0 []\n"
     ]
    }
   ],
   "source": [
    "apples = fruits.filter(lambda x: \"apple\" in x)\n",
    "lemons = yellowThings.filter(lambda x: \"lemon\" in x)\n",
    "applesAndLemons = apples.union(lemons)\n",
    "print(applesAndLemons.collect())\n",
    "print(applesAndLemons.toDebugString().decode(\"utf-8\")) # decode used for nice formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_6jZlHQt2E8-"
   },
   "source": [
    "### 3.2 Exercise\n",
    "\n",
    "1. What does the code below do?\n",
    "1. Draw the linage graph for the code\n",
    "1. List actions and transformations used in it\n",
    "1. When are all computations executed?\n",
    "1. If we call `result.collect()` again, what will Spark do to perform the action? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 1284.60205078125,
      "end_time": 1604572161885.815
     }
    },
    "id": "naDnHCH12E8_",
    "outputId": "5da8b1db-dd40-4c97-9bd8-7851b4b062c5",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('apple', 1),\n",
       " ('banana', 1),\n",
       " ('canary', 1),\n",
       " ('melon', 1),\n",
       " ('grape', 1),\n",
       " ('lemon', 1),\n",
       " ('orange', 1),\n",
       " ('pineapple', 1),\n",
       " ('strawberry', 1)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = sc.textFile('fruits.txt')\n",
    "words = text.flatMap(lambda x: x.split(\" \"))\n",
    "result = words.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y)\n",
    "result.saveAsTextFile('result')\n",
    "result.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QLO-mxZJ2E9B"
   },
   "source": [
    "### 3.3 Persistence (Caching)\n",
    "\n",
    "Spark's RDDs are by default recomputed each time you run an action on\n",
    "them. If you would like to reuse an RDD in multiple actions, you can ask Spark to persist it using `RDD.persist()`. After computing it the first time, Spark will store the RDD contents in memory (partitioned across the machines in your cluster), and reuse them in future actions. Persisting RDDs on disk instead of memory is also possible.\n",
    "\n",
    "If you attempt to cache too much data to fit in memory, Spark will automatically evict old partitions using a Least Recently Used (LRU) cache policy. For the memory-only storage levels, it will recompute these partitions the next time they are accessed,\n",
    "while for the memory-and-disk ones, it will write them out to disk. In either case, this means that you don't have to worry about your job breaking if you ask Spark to cache too much data. However, caching unnecessary data can lead to eviction of useful data\n",
    "and more recomputation time. Finally, RDDs come with a method called `unpersist()` that lets you manually remove them from the cache.\n",
    "\n",
    "Please note that both `persist()` and `cache()` (which is a simple wrapper that calls `persist(storageLevel=StorageLevel.MEMORY_ONLY)` - see [here](https://spark.apache.org/docs/latest/api/python/_modules/pyspark/rdd.html#RDD.cache) for details -) are lazy operations themselves. The caching operation will, in fact, only take place when the first action is called. With successive action calls, the cached RDD will be used.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MrhC0T7V2E9B"
   },
   "source": [
    "### 3.4 Exercise:\n",
    "1. Write some code which can benefit from caching.\n",
    "1. Where should we ask Spark to persist the RDD in Exercise 3.2 to prevent it from re-executing the code when we call `collect()` again?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7bTnnUpiMtE7"
   },
   "source": [
    "#### Explicit Caching of RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.storagelevel import StorageLevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 19905.183837890625,
      "end_time": 1604569866420.997
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "id": "dd9qx9Id2E9C",
    "outputId": "4185cb73-63d8-409f-904b-53f73af02edf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, ['apple', 'banana', 'canary melon'])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#* Example 1\n",
    "fruits.cache()\n",
    "fruits.count(), fruits.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, 'Memory Serialized 1x Replicated')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#* Memory-cached deserialized object + (Memory-cached) serialized Java object\n",
    "fruits.is_cached, str(fruits.getStorageLevel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, 'Serialized 1x Replicated')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fruits.unpersist()\n",
    "#* Memory-cached serialized Java object\n",
    "fruits.is_cached, str(fruits.getStorageLevel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 2326.470947265625,
      "end_time": 1604569882117.9
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "id": "mmU2tW5W2E9D",
    "outputId": "6d273032-274a-439c-c120-f7e50d973535"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('apple', 1),\n",
       " ('banana', 1),\n",
       " ('canary', 1),\n",
       " ('melon', 1),\n",
       " ('grape', 1),\n",
       " ('lemon', 1),\n",
       " ('orange', 1),\n",
       " ('pineapple', 1),\n",
       " ('strawberry', 1)]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#* Example 2\n",
    "words = fruits.flatMap(lambda x: x.split(\" \"))\n",
    "result = words.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y)\n",
    "result.persist(StorageLevel.MEMORY_AND_DISK_2)\n",
    "result.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, 'Disk Memory Serialized 2x Replicated')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#* Disk (if falls out of mem) +\n",
    "#* Memory-cached deserialized object +\n",
    "#* Memory-cached serialized Java object +\n",
    "#* Replicated on 2 nodes\n",
    "result.is_cached, str(result.getStorageLevel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, 'Serialized 1x Replicated')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.unpersist()\n",
    "result.is_cached, str(result.getStorageLevel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Zuey7WN2E9F"
   },
   "source": [
    "### 3.5 Working with Key/Value Pairs\n",
    "\n",
    "\n",
    "Spark provides special operations on RDDs containing key/value pairs. These RDDs\n",
    "are called *pair RDDs*. Pair RDDs are a useful building block in many programs, as\n",
    "they expose operations that allow you to act on each key in parallel or regroup data\n",
    "across the network. For example, pair RDDs have a `reduceByKey()` method that can\n",
    "aggregate data separately for each key, and a `join()` method that can merge two\n",
    "RDDs together by grouping elements with the same key. Pair RDDs are also still RDDs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 811.181884765625,
      "end_time": 1604570014682.289
     }
    },
    "id": "p__T2_bF2E9G",
    "outputId": "d8bf7fa1-273c-42bd-9b7e-7b500268ae2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('key1', (0, 1)), ('key2', (3, 1)), ('key1', (8, 1)), ('key3', (3, 1)), ('key3', (9, 1))]\n",
      "[('key1', (8, 2)), ('key2', (3, 1)), ('key3', (12, 2))]\n",
      "(1) PythonRDD[199] at collect at <ipython-input-69-f780a7e4e5c2>:6 []\n",
      " |  MapPartitionsRDD[197] at mapPartitions at PythonRDD.scala:145 []\n",
      " |  ShuffledRDD[196] at partitionBy at NativeMethodAccessorImpl.java:0 []\n",
      " +-(1) PairwiseRDD[195] at reduceByKey at <ipython-input-69-f780a7e4e5c2>:4 []\n",
      "    |  PythonRDD[194] at reduceByKey at <ipython-input-69-f780a7e4e5c2>:4 []\n",
      "    |  ParallelCollectionRDD[193] at readRDDFromFile at PythonRDD.scala:274 []\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "rdd = sc.parallelize([(\"key1\", 0) ,(\"key2\", 3), (\"key1\", 8) ,(\"key3\", 3), (\"key3\", 9)])\n",
    "rdd2 = rdd.mapValues(lambda x: (x, 1))\n",
    "rdd3 = rdd2.reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "print(rdd2.collect())\n",
    "print(rdd3.collect())\n",
    "print(rdd3.toDebugString().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vSGny_-F2E9I"
   },
   "source": [
    "### 3.6 Exercise\n",
    "1. What does the code above do? \n",
    "2. Where can it be used? Complete the code to perform the desired functionality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 1054.051025390625,
      "end_time": 1604570191639.609
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 214
    },
    "id": "4Sbh0ffn2E9J",
    "outputId": "16e171ea-d26c-4f4b-a868-e980b5f54a0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('key1', 4.0), ('key2', 3.0), ('key3', 6.0)]\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize([(\"key1\", 0) ,(\"key2\", 3),(\"key1\", 8) ,(\"key3\", 3),(\"key3\", 9)])\n",
    "rdd2 = rdd.mapValues(lambda x: (x, 1)).reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "average = rdd2.mapValues(lambda x: x[0] / x[1])\n",
    "print(average.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h-kni-P32E9L"
   },
   "source": [
    "### 3.7 Spark Partitioning\n",
    "Spark programs can choose to control their RDDs' partitioning\n",
    "to reduce communication. Partitioning will not be helpful in all applications, for\n",
    "example, if a given RDD is scanned only once, there is no point in partitioning it in\n",
    "advance. It is useful only when a dataset is reused multiple times in key-oriented\n",
    "operations such as joins.\n",
    "\n",
    "Spark's partitioning is available on all RDDs of key/value pairs, and causes the system\n",
    "to group elements based on a function of each key. Although Spark does not give\n",
    "explicit control of which worker node each key goes to (partly because the system is\n",
    "designed to work even if specific nodes fail), it lets the program ensure that a set of\n",
    "keys will appear together on some node.\n",
    "\n",
    "\n",
    "Many of Spark's operations involve shuffling data by key across the network. All of\n",
    "these will benefit from partitioning. Examples of operations that benefit from\n",
    "partitioning are `cogroup()`, `groupWith()`, `join()`, `leftOuterJoin()`, `rightOuterJoin()`, `groupByKey()`, `reduceByKey()`, `combineByKey()`, and `lookup()`.\n",
    "\n",
    "By default PySpark uses hash partitioning as the partitioning function. A way to define a custom partition is by using the function `partitionBy()`. To use `partitionBy()` the RDD must consist of tuple objects. This function is a transformation, therefore a new RDD will be returned. In the following example we are going to see a default partitioning scheme of Spark as well as a custom partitioning.\n",
    "\n",
    "Partitioning allows some Spark code to run more efficiently, in particular running 'pair' operations on pair RDD (eg. mapValues, reduceByKey) is guaranteed to produce no shuffling in the cluster and also preserve the partitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why do we need partitioning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 94.587890625,
      "end_time": 1604570600597.958
     }
    },
    "id": "9RD8T7Eo2E9O"
   },
   "outputs": [],
   "source": [
    "nums = [(k, str(v)) for k in range(3) for v in range(2)]\n",
    "pairs = sc.parallelize(nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 296.9658203125,
      "end_time": 1604570604299.245
     }
    },
    "id": "AavpwTqr2E9P",
    "outputId": "8c34d7aa-c04d-40d8-9dd4-2d59331a8547"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions:  1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[(0, '0'), (0, '1'), (1, '0'), (1, '1'), (2, '0'), (2, '1')]]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Number of partitions: \", pairs.getNumPartitions())\n",
    "pairs.glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hash partitioning (default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 301.080078125,
      "end_time": 1604570637324.732
     }
    },
    "id": "GVkt8g302E9R"
   },
   "outputs": [],
   "source": [
    "pairs = sc.parallelize(nums).partitionBy(3, lambda k: hash(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 791.9169921875,
      "end_time": 1604570650694.457
     }
    },
    "id": "z0QzAt302E9U",
    "outputId": "68b802a5-9765-46a7-cb21-ed15bfd7faf4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions:  3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[(0, '0'), (0, '1')], [(1, '0'), (1, '1')], [(2, '0'), (2, '1')]]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Number of partitions: \", pairs.getNumPartitions())\n",
    "pairs.glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Range partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 301.080078125,
      "end_time": 1604570637324.732
     }
    },
    "id": "GVkt8g302E9R"
   },
   "outputs": [],
   "source": [
    "pairs = sc.parallelize(nums).partitionBy(3, lambda k: k > 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 791.9169921875,
      "end_time": 1604570650694.457
     }
    },
    "id": "z0QzAt302E9U",
    "outputId": "68b802a5-9765-46a7-cb21-ed15bfd7faf4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions:  3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[(0, '0'), (0, '1'), (1, '0'), (1, '1')], [(2, '0'), (2, '1')], []]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Number of partitions: \", pairs.getNumPartitions())\n",
    "pairs.glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xZCEMl182E9V"
   },
   "source": [
    "### 3.8 Converting a user program into tasks\n",
    "\n",
    "A Spark driver is responsible for converting a user program into units of physical execution called tasks. At a high level, all Spark programs follow the same structure: they create RDDs from some input, derive new RDDs from those using transformations, and perform actions to collect or save data. A Spark program implicitly creates a logical **directed acyclic graph (DAG)** of operations.\n",
    "When the driver runs, it converts this logical graph into a physical execution plan.\n",
    "\n",
    "Spark performs several optimizations, such as \"pipelining\" map transformations together to merge them, and converts the execution graph into a set of **stages**.\n",
    "Each stage, in turn, consists of multiple tasks. The tasks are bundled up and prepared to be sent to the cluster. Tasks are the smallest unit of work in Spark; a typical user program can launch hundreds or thousands of individual tasks.\n",
    "\n",
    "Each RDD maintains a pointer to one or more parents along with metadata about what\n",
    "type of relationship they have. For instance, when you call `val b = a.map()` on an\n",
    "RDD, the RDD `b` keeps a reference to its parent `a`. These pointers allow an RDD to be\n",
    "traced to all of its ancestors.\n",
    "\n",
    "The following phases occur during Spark execution:\n",
    "* User code defines a DAG (directed acyclic graph) of RDDs. Operations on RDDs create new RDDs that refer back to their parents, thereby creating a graph.\n",
    "* Actions force translation of the DAG to an execution plan. When you call an action on an RDD, it must be computed. This requires computing its parent RDDs as well. \n",
    "* Spark's scheduler submits a job to compute all needed RDDs. That job will have one or more stages, which are parallel waves of computation composed of tasks. Each stage will correspond to one or more RDDs in the DAG. A single stage can correspond to multiple RDDs due to pipelining.\n",
    "* Tasks are scheduled and executed on a cluster\n",
    "* Stages are processed in order, with individual tasks launching to compute segments of the RDD. Once the final stage is finished in a job, the action is complete.\n",
    "\n",
    "If you visit the application's web UI (http://localhost:4040/jobs/), you will see how many stages occur in order to\n",
    "fulfill an action. For more details about the content of this page, see [Spark job debugging](https://docs.microsoft.com/en-us/azure/hdinsight/hdinsight-apache-spark-job-debugging) for Azure Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FD9QH0PY2E9W",
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "### 3.9 Exercise. \n",
    "\n",
    "1. Why is Spark faster than Hadoop MapReduce?\n",
    "1. Study the examples above via Spark UI. Observe how many stages they have. \n",
    "1. Which of the graphs below are DAGs?\n",
    "\n",
    "<img src=\"https://cloud.inf.ethz.ch/s/kkes72cJjsoHkbY/download\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hH7j0cAO2E9W"
   },
   "source": [
    "### 3.10 True or False\n",
    "Say if the following statements are *true* or *false*, and explain why.\n",
    "\n",
    "1. Each RDD is split into multiple partitions, which may be computed on different nodes of the cluster.\n",
    "\n",
    "    True\n",
    "\n",
    "1. Transformations construct a new RDD from a previous one and immediately calculate the result\n",
    "\n",
    "    False\n",
    "    \n",
    "1. Spark's RDDs are by default recomputed each time you run an action on them\n",
    "\n",
    "    True???\n",
    "\n",
    "1. After computing an RDD, Spark will store its contents in memory and reuse them in future actions.\n",
    "    \n",
    "    False\n",
    "    \n",
    "1. When you derive new RDDs using transformations, Spark keeps track of the set of dependencies between different RDDs.\n",
    "\n",
    "    True\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equivalence: function calls VS SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difference between RDD, DF, DS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Th3qkpTo2E9X"
   },
   "source": [
    "## 4. TF-IDF in Spark (OPTIONAL)\n",
    "In this exercise you will implement a simple query engine over the Gutenberg dataset using Spark.\n",
    "The [Gutenberg dataset](https://www.gutenberg.org/) consists of 3036 free ebooks. The goal of this exercise is to develop a search engine to find the most relevant books given a text query.\n",
    "\n",
    "### 4.1 Get the data\n",
    "1. You can download the dataset (the smallest one) from: https://zenodo.org/record/3360392\n",
    "\n",
    "2. Unzip and put all .txt files under a directory gutenberg\n",
    "\n",
    "3. docker cp gutenberg jupyter:/home/jovyan/work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8kxrDBCi2E9X"
   },
   "source": [
    "### 4.2 Understand TF-IDF\n",
    "\n",
    "[TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) is a statistic to determine the relative importance of the words in a set of documents. Is is computed as the product of two statistics, term frequency (`tf`) and inverse document frequency (`idf`). \n",
    "\n",
    "Given a word `t`, a document `d` (in this case, a book) and the collection of all documents `D` we can define `tf(t, d)` as the number of times `t` appears in `d`. This gives us some information about the content of a document but because some terms (eg. \"the\") are so common, term frequency will tend to incorrectly emphasize documents which happen to use the word \"the\" more frequently, without giving enough weight to the more meaningful terms.\n",
    "\n",
    "The inverse document frequency `idf(t, D)` is a measure of how much information the word provides, that is, whether the term is common or rare across all documents. It can be computed as:\n",
    "\n",
    "<img src=\"https://cloud.inf.ethz.ch/s/gw25WWcbd9iXBdK/download\" width=\"300\">\n",
    "\n",
    "where $|D|$ is the total number of documents and the denominator represents how many documents contain the word $t$ at least once. However, this would cause a division-by-zero exception if the user query a word that never appear in the dataset. A better formulation would be:\n",
    "\n",
    "<img src=\"https://cloud.inf.ethz.ch/s/fXffB5g59y3y2an/download\" width=\"300\">\n",
    "\n",
    "Then, the `tdidf(t, d, D)` is calculated as follows:\n",
    "\n",
    "<img src=\"https://cloud.inf.ethz.ch/s/2dAsg3k2QaL3XMz/download\" width=\"300\">\n",
    "\n",
    "A high weight in `tfidf` is reached by a high term frequency (in the given document) and a low document frequency of the term in the whole collection of documents.\n",
    "\n",
    "Having already implemented TF-IDF last week in pseudocode, in this week we are going to implement it in Spark. The following code snippet imports the whole dataset into an RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 362.551025390625,
      "end_time": 1604572323489.335
     }
    },
    "id": "cBgZ_ztV2E9X"
   },
   "outputs": [],
   "source": [
    "# sc is automatically defined as SparkContext\n",
    "# docs will be an RDD in the format [(docName, content)]\n",
    "docs = sc.wholeTextFiles(\"gutenberg/*.txt\", minPartitions=100)\n",
    "\n",
    "# number of documents in the folder\n",
    "docs_number = docs.count()\n",
    "\n",
    "# display the [(docName, content)] values\n",
    "#docs.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tnD0z1DA2E9Z"
   },
   "source": [
    "#### TF-IDF solution code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [
    "8r-Wtteo2E8d",
    "K9y3brO92E8d",
    "oNbMFPQm2E8d",
    "KEb2NxyP2E8e",
    "QLO-mxZJ2E9B",
    "xZCEMl182E9V",
    "FD9QH0PY2E9W",
    "EPnOr09W2E9W"
   ],
   "name": "Exercise08_Spark_Solution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
