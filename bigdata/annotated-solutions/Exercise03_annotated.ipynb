{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Big Data &ndash; Exercises</center>\n",
    "## <center>Fall 2023 &ndash; Week 3 &ndash; ETH Zurich</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Go through (interesting) questions in this problem set\n",
    "1. Bonus questions\n",
    "1. Answer your questions (lectures/quizzes/hadoop CLI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This week we will cover mostly theoretical aspects of Hadoop and HDFS and we will discuss advantages and limitations of different storage models.\n",
    "\n",
    "#### What is Hadoop?\n",
    "Hadoop provides a **distributed file system** and a\n",
    "**framework for the analysis and transformation** of very **large**\n",
    "data sets using the MapReduce paradigm.\n",
    "\n",
    "Several components are part of this framework. In this course you will study HDFS, MapReduce and HBase while this exercise focuses on HDFS and storage models.\n",
    "\n",
    "\n",
    "| *Component*                |*Description*  |*First developer*  |\n",
    "|----------------------------------------------|---|---|\n",
    "| **HDFS**                  |Distributed file system  |Yahoo!  |\n",
    "| **MapReduce**   |Distributed computation framework   |Yahoo!  |\n",
    "| **HBase**           | Column-oriented table service  |Powerset (Microsoft)  |\n",
    "| Pig  | Dataflow language and parallel execution framework  |Yahoo!   |\n",
    "| Hive            |Data warehouse infrastructure   |Facebook  |\n",
    "| ZooKeeper    |Distributed coordination service   |Yahoo!  |\n",
    "| Chukwa  |System for collecting management data   |Yahoo!  |\n",
    "| Avro                |Data serialization system   |Yahoo! + Cloudera  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. The Hadoop Distributed File System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.1 &ndash; State which of the following statements are true:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### The HDFS namespace is a hierarchy of files and directories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    True: Namespace==file system hierarchy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### In HDFS, each block of the file is either 64 or 128 megabytes depending on the version and distribution of Hadoop in use, and this *cannot* be changed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    False: This can be customized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### A client wanting to write a file into HDFS, first contacts the NameNode, then sends the data to it. The NameNode will write the data into multiple DataNodes in a pipelined fashion. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    False: Data blocks never go through the NameNode. Clients directly communicate with the DataNodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### A DataNode may execute multiple application tasks for different clients concurrently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    True: It's just a process, which ofc can do multi-tasking/-threading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### The cluster can have thousands of DataNodes and tens of thousands of HDFS clients per cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### HDFS NameNodes keep the namespace both in DRAM and on disk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    True: Keeping the state of the distributed DB persisted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### The locations of block replicas are part of the persistent checkpoint that the NameNode stores in its native file system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    False: The block-to-DataNode mapping can be constructed at runtime from BlockReports."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Bonus questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Client can customize the replica count for each data block of a file given their importance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    False: The target replica count is set differently per file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Storing a 170 MB file will eventually require 256 MB of physical memory (2 blocks of 128 MB each)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    False: HDFS doesn't round up to the nearest block size when storing files.Â So, if you have a 170 MB file, it will be divided into two blocks - one block of 128 MB and another block of 42 MB (not another full 128 MB block). Therefore, it requires 170 MB of storage space, not 256 MB.\n",
    "    It's important to note that this doesn't take into account replication, which is a process in HDFS where data is copied to other nodes in the system to ensure data reliability. By default, HDFS makes 3 copies of each block. So if replication is considered, the 170 MB file would consume 510 MB of storage space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Individual DataNodes are not fault-tolerant but the system as a whole is. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    True: Individual DataNodes are not replicated using e.g., RAID (redundant array of independent/inexpensive disks)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### NameNode does not store any data blocks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### NameNode alone has sufficient information to service a client accessing the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    True: NameNode has all the metadata it needs; it doesn't need to serve the real data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### NameNode was the single point of failure in the first version of HDFS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    True: Only a sinlge NameNode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### The client computes a checksum of each block of the file and stores these checksums in a separate hidden file in the same HDFS namespace. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    True: The client recomputes the checksum for every block it receives and then compares the new checksum with the one the DataNode sent. This computation can be done efficiently, since it is a very common operation and is, therefore, supported by hardware already."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### HDFS provides only limited random access capability as clients can read off a contiguous subset of bytes of the entire block. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    False: This is NOT random access and is a slow operation.\n",
    "    The definition of random access is accessing any location in constant time --- DRAM is a grid consists of rows and columns of capacitors, so you can index into any memory cell in consitant time by providing the row and column address. \n",
    "    (Reading a row is much faster than reading a column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### When writing a file, NameNode will send the client all the locations of the corresponding DataNodes sorted by their distances to the client in ascending order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### DataNodes and NameNode are typically physical machines running in datacenters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    False: DataNodes and NameNode are essentially processes (software) running on (virtual) machines in datacenters or just our laptops."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### HDFS satisfies the CAP requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    True: Only consistency and availability, but not partition tolerance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### When reading and writing files, data blocks never go through the NameNode; they are transferred only among the client and DataNodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Client can customize the replica count for each data block of a file given their importance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    False: The target replica count is set per file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Edit log contains only the operations after when the Namespace file was created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Observer NameNodes can sometimes be leveraged to increase the file writing throughput."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Observer NameNodes can be used to increase the throughput ofÂ **reading**Â files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Sending and receiving of Heartbeats is synchronous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    False: The NameNode dosen't wait for the Heartbeats, which are sent periodically every 3 s by default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "### 1.2 &ndash; A typical file system block size is 4096 bytes. How large is a block in HDFS? List at least two advantages of such a choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**\n",
    "\n",
    "Typical size for a block is either 64 or 128 megabytes. A large block size offers several important advantages. \n",
    "\n",
    "1. **It minimizes the cost of seeks.** If the block is large enough, the time it takes to transfer the data from the disk can be significantly longer than the time to seek to the start of the block. Thus, transferring a large file made of multiple blocks operates at the disk transfer rate.\n",
    "\n",
    "2. It reduces clients' need to interact with the NameNode because reads and writes on the same block require only **one initial request to the NameNode for location information.** The reduction is significant for workloads where applications mostly read and write large files sequentially. \n",
    "\n",
    "3. Since on a large block, a client is more likely to perform many operations on the given block, it can reduce network overhead by **keeping a persistent TCP connection to the DataNode over an extended period of time.** \n",
    "\n",
    "4. It **reduces the size of the metadata stored on the NameNode.** This allows us to keep the metadata in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 &ndash; How does the hardware cost grow as function of the amount of data we need to store in a Distributed File System such as HDFS? Why?\n",
    "\n",
    "\n",
    "**Solution**\n",
    "\n",
    "**Linearly**. HDFS is designed taking machine failure into account, and therefore DataNodes do not need to be (highly expensive) highly reliable machines. Instead standard commodity hardware can be used. Moreover the number of nodes can be increased as soon as it becomes necessary, avoiding wasting of resources when the amount of data is still limited. This is indeed the main advantage of scaling out compared to scaling up, which has exponential cost growth.\n",
    "\n",
    "\n",
    "### 1.4 &ndash; Single Point of Failure\n",
    "\n",
    "1. Which component is the main single point of failure in Hadoop?\n",
    "\n",
    "1. What is the Secondary NameNode?\n",
    "\n",
    "**Solution**\n",
    "\n",
    "\n",
    "1. Prior to Hadoop 2.0.0, the **NameNode was a single point of failure**. While the loss of any other machine (intermittently or permanently) does not result in data loss, NameNode loss results in cluster unavailability. The permanent loss of NameNode data would render the cluster's HDFS inoperable.\n",
    "The HDFS High Availability feature addresses the above problems by providing the option of running two redundant NameNodes in the same cluster in an Active/Passive configuration with a hot standby. \n",
    "\n",
    "1. The Secondary NameNode is a node that merges the fsimage and the edits log files periodically and keeps edits log size within a limit. This allows the NameNode to start up faster in case of failure, but the Secondary NameNode is not a redundant NameNode. Over the years, the HDFS team kept improving on the \"alternative\" name nodes and came up almost every year with a new name with new functionality improving on the former ones. In the lecture, we discuss the latest up-to-date variant, standby namenodes, saying all the others (secondary, checkpoint, backup...) are just \"HDFS archeology\". For the possible configurations of namenodes, see [the official HDFS user guide](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html#Secondary_NameNode). \n",
    "\n",
    "- Checkpoint Node\n",
    "- Backup Node\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 &ndash; Scalability, Durability and Performance on HDFS\n",
    "Explain how HDFS accomplishes the following requirements:\n",
    "\n",
    "1. Scalability\n",
    "\n",
    "1. Durability\n",
    "\n",
    "1. High sequential read/write performance\n",
    "\n",
    "**Solution**\n",
    "\n",
    "1. Scalability: by partitioning files into blocks and distributing them to many servers operating in parallel, HDFS can scale to potentially a large number of files of any size. By adding more DataNodes the storage capacity of the system can be increased arbitrarily. It has been demonstrated to scale beyond tens of petabytes (PB). More importantly, it does so with linear performance characteristics and cost.\n",
    "\n",
    "1. Durability: HDFS creates multiple copies of each block (by default 3, on different racks) to minimize the probability of data loss.\n",
    "\n",
    "1. High sequential read/write performance: by splitting huge files into blocks and spreading these into multiple machines. This makes parallel reads possible (accessing different nodes at the same time) either by using multiple clients or by using a distributed data processing framework such as MapReduce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. File I/O operations and replica management.\n",
    "\n",
    "\n",
    "### 2.1 &ndash; Replication policy\n",
    "Assume your HDFS cluster is made of 3 racks, each containing 3 DataNodes. Assume also the HDFS is configured to use a block size of 100 megabytes and that a client is connecting from outside the datacenter (therefore no DataNode is priviledged). \n",
    "\n",
    "1. The client uploads a file of 150 megabytes. Draw in the picture below a possible blocks configuration according to the default HDFS replica policy. How many replicas are there for each block? Where are these replicas stored?\n",
    "\n",
    "1. Can you find a different policy that, using the same number of replicas, improves the expected availability of a block? Does your solution show any drawbacks?\n",
    "\n",
    "1. Referring to the picture below, assume a block is stored in Node 3, as well as in Node 4 and Node 5. If this block of data has to be processed by a task running on Node 6, which of the three replicas will be actually read by Node 6? \n",
    "\n",
    "<img src=\"https://polybox.ethz.ch/index.php/s/lRzwDdtmytzyDRR/download\" width=\"500\">\n",
    "\n",
    "**Solution**\n",
    "\n",
    "1. For each block independently, the HDFS's placement policy is to put one replica on a random node in a random rack, another on one node in a different rack, and the last on a different node in the same rack chosen for the second replica. A possibile configuration is shown in the picture (but there are many more valid solutions).\n",
    "\n",
    "1. One could decide to store the 3 replicas in 3 different racks, increasing the expected availability. However this would also slow down the writing process that would involve two inter-rack communications instead of one. Usually, the probability of failure of an entire rack is much smaller than the probability of failure of a node and therefore it is a good tradeoff to have 2/3 of the replicas in one rack.\n",
    "\n",
    "1. Either the one stored in Node 4 or Node 5, assuming the intra-rack topology is such that the distance from these nodes to Node 6 is the same. In general, the reading priority is only based on the distance, not on which node was first selected in the writing process.\n",
    "\n",
    "<img src=\"https://polybox.ethz.ch/index.php/s/7GSTXm0caYreggq/download\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.2 &ndash; File read and write data flow.\n",
    "To get an idea of how data flows between the client interacting with HDFS, consider a diagram below which shows main components of HDFS. \n",
    "\n",
    "<img src=\"https://polybox.ethz.ch/index.php/s/R7hg8x7YEyTFPvD/download\" width=\"600\">\n",
    "\n",
    "1. Draw the main sequence of events when a client copies a file to HDFS.\n",
    "2. Draw the main sequence of events when a client reads a file from HDFS.\n",
    "3. Why do you think a client writes data directly to datanodes instead of sending it through the namenode?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Solution__\n",
    "\n",
    "\n",
    "1 - Steps 2-5 are applied for each block of the file. <br>\n",
    "   1. HDFS client asks the Namenode to create the file.\n",
    "   2. HDFS client asks the Namenode for a DataNode to host replica of the i-th block of the file. <br>\n",
    "   3. NameNode replies with a list of DataNodes and their locations for i-th block. <br>\n",
    "   4. The client writes i-th block to DataNodes in pipeline fashion. <br>\n",
    "       __NB__: \n",
    "           \n",
    "           â¢ The client only need to contact the 1st DataNode on the list.\n",
    "           â¢ Then the 1st DataNode will organize a writing pipeline consisting 3 DataNodes (by default) including itself.\n",
    "       \n",
    "   5. DataNodes in the write pipeline acknowledge the writing of a block. Once all of them replied, the first contacted DataNode replies with acknowledgement to the client. <br>\n",
    "       __NB__: \n",
    "           \n",
    "           â¢ Acknowledgement â  successful writing.\n",
    "           â¢ It just means that they've received the requests.\n",
    "   6. The client sends to the NameNode a request to close the file and release the lock. <br>\n",
    "   7. The DataNodes check with the NameNode for minimal replication. <br>\n",
    "       __NB__: \n",
    "           \n",
    "           â¢ Minimal number of replication is 1 in HDFS by default (i.e., 1 copy of every block of a file)\n",
    "           â¢ 2 in DynamoDB\n",
    "   8. The NameNode sends ack to the client on finishing writing the file. <br>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"https://polybox.ethz.ch/index.php/s/CvO26FssBV8eQ2M/download\" width=\"500\">\n",
    "\n",
    "2 -  \n",
    "   1. HDFS client request a file <br>\n",
    "   2. NameNode replies with a list of blocks and the locations of each replica. <br>\n",
    "   3. The client reads each block from the closest datanode.\n",
    "\n",
    "<img src=\"https://polybox.ethz.ch/index.php/s/zxoqGqIIpvAg3Qv/download\" width=\"500\">\n",
    "\n",
    "3 - If the namenode was responsible for copying all files to datanodes, then it would become a bottleneck."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.3 &ndash; Network topology.\n",
    "HDFS estimates the network bandwidth between two nodes by their distance. The distance from a node to its parent node is assumed to be one. A distance between two nodes can be calculated by summing up thier distances to their closest common ancestor. A shorter distance between two nodes means that the greater bandwidth they can utilize to transfer data. Consider a diagram of a possible hadoop cluster over two datacenters below. \n",
    "\n",
    "<img src=\"https://polybox.ethz.ch/index.php/s/Mk2kI7dkKZNrxul/download\" width=\"700\">\n",
    "\n",
    "Calculate following distances using the distance rule explained above:\n",
    "1. Node 0 and Node 1\n",
    "2. Node 0 and Node 2\n",
    "3. Node 1 and Node 4\n",
    "4. Node 4 and Node 5\n",
    "5. Node 2 and Node 3\n",
    "6. Two processes of Node 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Tips: Distance between two nodes that are__\n",
    "\n",
    "- Within a node (IPC): 0\n",
    "- Within a rack, between nodes: 2\n",
    "- Within a datacenter, between racks: 4\n",
    "- Between datacenters: 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Storage models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.1 &ndash; List two differences between Object Storage and Block Storage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Additional Answers__\n",
    "\n",
    "Object storage and block storage are distinct storage paradigms used in computing and data storage systems. Here's a comprehensive list highlighting their key differences:\n",
    "\n",
    "1. **Data Unit and Structure**:\n",
    "   - **Object Storage**: Data is stored as objects, including the data, metadata (descriptive information), and a unique identifier. Objects are stored in a flat hierarchy.\n",
    "   - **Block Storage**: Data is stored in fixed-sized blocks (e.g., 4KB, 8KB, 512B) and does not contain metadata or unique identifiers within the storage system.\n",
    "\n",
    "1. **Access Method**:\n",
    "   - **Object Storage**: Accessed via HTTP(S) with RESTful APIs, allowing metadata-based retrieval and management (e.g., Amazon S3, Azure Blob Storage).\n",
    "   - **Block Storage**: Accessed using block-level protocols such as iSCSI (Internet Small Computer System Interface) and is managed at the operating system level.\n",
    "\n",
    "1. **Use Cases**:\n",
    "   - **Object Storage**: Suitable for storing unstructured data like media files, backups, and large-scale web applications. Highly scalable for big data and cloud applications.\n",
    "   - **Block Storage**: Ideal for structured data, databases, virtual machines, and applications requiring low-latency, high-performance access.\n",
    "\n",
    "1. **Metadata and Searchability**:\n",
    "   - **Object Storage**: Provides rich metadata for each object, enabling efficient indexing and search capabilities.\n",
    "   - **Block Storage**: Lacks inherent metadata associated with individual blocks, making it less searchable at the storage level.\n",
    "\n",
    "1. **Cost Efficiency**:\n",
    "   - **Object Storage**: Generally more cost-effective for storing large amounts of data due to its scalability and optimized storage techniques.\n",
    "   - **Block Storage**: Tends to be more expensive on a per-unit basis, making it suitable for performance-critical applications.\n",
    "\n",
    "1. **Parallel Operations**:\n",
    "   - **Object Storage**: Enables concurrent operations on different objects, allowing for better parallelism and scalability.\n",
    "   - **Block Storage**: Limited parallelism as it operates on blocks and lacks the ability to directly access multiple blocks simultaneously.\n",
    "\n",
    "1. **Transaction Management**:\n",
    "   - **Object Storage**: Often has built-in transaction support for atomic operations on objects, aiding in data consistency.\n",
    "   - **Block Storage**: Lacks inherent transaction support for atomic operations on blocks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 &ndash; Compare Object Storage and Block Storage. For each of the following use cases, say which technology better fits the requirements.\n",
    "\n",
    "1. Store Netflix movie files in such a way they are accessible from many client applications at the same time [ *Object storage | Block Storage* ]\n",
    "    Block\n",
    "\n",
    "1. Store experimental and simulation data from CERN [ *Object storage | Block Storage* ]\n",
    "    Block\n",
    "\n",
    "1. Store the auto-backups of iPhone/Android devices [ *Object storage | Block Storage* ]\n",
    "    Object\n",
    "    \n",
    "**Solution**\n",
    "\n",
    "1. **Object Storage**. The movies are not excessively large to require Block Storage, while they can indefinitely in number, also a simple key-value model is enough without requiring a file-system hierarchy.\n",
    "\n",
    "1. **Block Storage**. Because it can handle large files and store more data than ordinary object storage.\n",
    "\n",
    "1. **Object Storage**. Backups are usually written once and rarely read. When data is read, partial access to each file is not essential. The client devices do not need to know the block composition of the object being stored. In fact, Apple [publicly confirmed](http://readwrite.com/2014/08/26/apple-icloud-amazon-web-services-hosting/) that backups data for iOS is stored on Amazon S3 and Microsoft Azure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Working with Docker-Hadoop\n",
    "\n",
    "Build and run the Hadoop docker image by `docker-compose up -d` in the `exercise03` directory. If completed successfully, you should be able to browse [`http://localhost:9870/`](http://localhost:9870/) and visualize the web interface of the daemon which should look similar to the following image. In the `Datanodes` tab you should see a single operating datanode.\n",
    "\n",
    "<img src=\"https://polybox.ethz.ch/index.php/s/LpWcGWZeU5mipBK/download\" width=\"800\">\n",
    "\n",
    "\n",
    "### Connecting to containers  \n",
    "\n",
    "Each Hadoop cluster is set up in one of the three supported modes:\n",
    "\n",
    "- Local (Standalone) Mode\n",
    "- Pseudo-Distributed Mode\n",
    "- Fully-Distributed Mode\n",
    "\n",
    "By default Hadoop runs in Local Mode but we will run it in the *Pseudo-Distributed Mode*. This will allow you to run Hadoop on a single-node (your computer) simulating a distributed file system, with datanode and namenode running in separate containers. For this excercise you will only need to connect to `namenode` and `datanode` containers. To connect to namenode container can use the Docker dashboard interface by navigating to `docker-hadoop` app, and selecting `CLI` option from the `namenode` container (see image below).\n",
    "\n",
    "<img src=\"https://polybox.ethz.ch/index.php/s/Hdlyhagx3JWbLBy/download\" width=\"700\">\n",
    "\n",
    "Alternatively, you can run `docker exec -it namenode /bin/bash` in a terminal. To connect to a datanode, you can similarly find it in the dashboard or run `docker exec -it namenode /bin/bash` in the terminal. Both approaches will give you shell access on the corresponding container. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 &ndash; Upload a file into HDFS\n",
    "\n",
    "Connect to the namenode by `docker exec -it namenode /bin/bash`.\n",
    "\n",
    "Pick an image file in your computer (or you can also download a random one) and try to upload it to HDFS. You may need to create an empty directory before uploading. (Check [here](https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/FileSystemShell.html) for help.)\n",
    "\n",
    "1. Which command do you use to upload from the local file system to HDFS?\n",
    "\n",
    "1. Which information can you find if you use `Utilities -> Browse the file system` in the daemon web interface?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 &ndash; Local File System\n",
    "\n",
    "1. ```bash\n",
    "   docker cp docker-compose.yml namenode:docker-compose.yml \n",
    "   ```\n",
    "Then, use HDFS commands to create a directory, copy the `docker-compose.yml` file from your local file system to HDFS. Use `cat` to check if the file is the same on the local and distributed systems. \n",
    "\n",
    "   *Hint:* you may use the following HDFS commands `-mkdir` for directory, `-copyFromLocal` for uploading the file, and `-cat` for printing them. You may have to first use `docker cp` to copy to file into the namenode container.\n",
    "\n",
    "2. Try to locate the file on a datanode. To connect to a datanode by running:\n",
    "\n",
    "   ```bash\n",
    "   docker exec -it datanode /bin/bash\n",
    "   ```\n",
    "\n",
    "   This will give you shell access to the data node machine. cd into `/hadoop/dfs/data/current/` directory and follow the directories until there are only files. Can you check if the file contents are the same as the one you uploaded? Use `ls -l` to check the size of the file size on the local \n",
    "\n",
    "3. Now try to upload a file to HDFS that is ~150MB. On Unix-based system you can also generate such a file filled with zero using:\n",
    "\n",
    "   ```bash\n",
    "   dd if=/dev/zero of=zeros.dat bs=1M count=150\n",
    "   ```\n",
    "\n",
    "   How many blocks the file is split into?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Demystifying FsImage & Edits, & Checkpoint\n",
    "\n",
    "When the NameNode starts up, or a checkpoint is triggered by a configurable threshold:\n",
    "\n",
    "- It reads the FsImage and EditLog from disk.\n",
    "- It applies all the transactions from the EditLog to the in-memory representation of the FsImage.\n",
    "- It flushes out this new version into a new FsImage on disk.\n",
    "- It truncates the old EditLog because its transactions have been applied to the persistent FsImage.\n",
    "\n",
    "A checkpoint can be triggered:\n",
    "\n",
    "> at a given time interval (dfs.namenode.checkpoint.period) expressed in seconds,\n",
    "> or after a given number of filesystem transactions have accumulated (dfs.namenode.checkpoint.txns).\n",
    "\n",
    "If both of these properties are set, the first threshold to be reached triggers a checkpoint.\n",
    "\n",
    "1. Query the configuration file\n",
    "\n",
    "   - `hdfs getconf -confKey dfs.namenode.checkpoint.period`\n",
    "   - `hdfs getconf -confKey dfs.namenode.checkpoint.txns`\n",
    "   - The fsimage & edit logs location `hdfs getconf -confKey dfs.namenode.name.dir`, I get something like `file:///hadoop/dfs/name`\n",
    "   - Find the fsimage and edit logs in the `current` directory. They must be named like `fsimage_0000000000000000000` & `edits_inprogress_0000000000000000001` \n",
    "   - Output edits `hdfs oev -p xml -i /hadoop/dfs/name/current/edits_inprogress_0000000000000000001 -o edits.xml `\n",
    "   - Output fsimage `hdfs oiv -p XML -i /hadoop/dfs/name/current/fsimage_0000000000000000000 -o fsimage.xml`\n",
    "\n",
    "2. Can you make sense of the outputs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Changing Block Size (optional)\n",
    "\n",
    "As explained in the tutorials, to change HDFS configurations you edit `etc/hadoop/core-site.xml` and `etc/hadoop/hdfs-site.xml`. In the docker app, you can modify the variables in the `hadoop.env`. For example, in the following line,\n",
    "\n",
    "```bash \n",
    "# hadoop.env \n",
    "CORE_CONF_fs_defaultFS=hdfs://namenode:9000\n",
    "```\n",
    "\n",
    "`CORE_CONF` corresponds to `core-site.xml`. The second part `fs_defaultFS=hdfs://namenode:9000` will be transformed into:\n",
    "\n",
    "```xml\n",
    "<property>\n",
    "    <name>fs.defaultFS</name><\n",
    "    value>hdfs://namenode:9000\n",
    "    </value>\n",
    "</property>\n",
    "```\n",
    "\n",
    "For more details [see here](https://github.com/big-data-europe/docker-hadoop).\n",
    "\n",
    "Try changing the default block size of HDFS to see its affect on read & write performance. You can change the block size by modifying the follwoing line in `hadoop.env`: `HDFS_CONF_dfs_block_size=1048576` The value `1048576` determines the block size in bytes, which in this case is `2^20 bytes` or 1 megabytes.\n",
    "\n",
    "> **_NOTE:_** that for these configuration changes to take effect you must restart the docker app!\n",
    "\n",
    "1. Create a file with size ~150MB and uploade the file to HDFS. Check number of blocks via the Web interface. \n",
    "\n",
    "2. For each of the following block sizes 1048576, 134217728, measure the time to transfer from local to HDFS, from HDFS to local, and removing the file. \n",
    "\n",
    "You can run the following commands \n",
    "```bash\n",
    "time hadoop fs -copyFromLocal zeros.dat /myfolder/zeros.dat\n",
    "time hadoop fs -get /myfolder/zeros.dat zeros.dat\n",
    "time hadoop fs -rm /myfolder/zeros.dat\n",
    "```\n",
    "Can you make sense of the results?\n",
    "\n",
    "> **_NOTE:_** make sure to remove the files before uploading them, so that no caching will distort the measurements"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc-showcode": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
